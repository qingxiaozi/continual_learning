import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import os


class ResultVisualizer:
    """ç»“æœå¯è§†åŒ–ç±»"""

    def __init__(self, save_dir="./results"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")

    def plot_training_loss(self, epoch_losses, save_plot=True, plot_name="training_loss.png"):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
        plt.figure(figsize=(10, 6))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        epochs = range(1, len(epoch_losses) + 1)
        plt.plot(epochs, epoch_losses, 'b-', linewidth=2, label='Training Loss')
        plt.scatter(epochs, epoch_losses, color='red', s=30, zorder=5)

        # è®¾ç½®å›¾è¡¨å±æ€§
        plt.title('Training Loss vs Epochs', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=10)

        # è®¾ç½®xè½´ä¸ºæ•´æ•°
        plt.xticks(epochs)

        # è‡ªåŠ¨è°ƒæ•´yè½´èŒƒå›´ï¼Œç¡®ä¿èƒ½çœ‹æ¸…ä¸‹é™è¶‹åŠ¿
        if len(epoch_losses) > 1:
            loss_range = max(epoch_losses) - min(epoch_losses)
            plt.ylim(min(epoch_losses) - 0.1 * loss_range,
                    max(epoch_losses) + 0.1 * loss_range)

        # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
        if save_plot:
            plot_path = os.path.join(self.save_dir, plot_name)
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"è®­ç»ƒæŸå¤±å›¾å·²ä¿å­˜è‡³: {plot_path}")

        plt.close()

        # æ‰“å°è®­ç»ƒæ€»ç»“
        if len(epoch_losses) > 1:
            self._print_training_summary(epoch_losses)

    def _print_training_summary(self, epoch_losses):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        print(f"\nè®­ç»ƒæ€»ç»“:")
        print(f"åˆå§‹æŸå¤±: {epoch_losses[0]:.4f}")
        print(f"æœ€ç»ˆæŸå¤±: {epoch_losses[-1]:.4f}")
        print(f"æŸå¤±ä¸‹é™: {epoch_losses[0] - epoch_losses[-1]:.4f}")
        print(f"ä¸‹é™ç™¾åˆ†æ¯”: {(epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100:.2f}%")

    def calculate_metrics(self, performance_history, time_history=None, communication_costs=None):
        """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}

        if time_history is None:
            time_history = []
        if communication_costs is None:
            communication_costs = []

        # æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        accuracies = [perf['accuracy'] for perf in performance_history if 'accuracy' in perf]
        losses = [perf['loss'] for perf in performance_history if 'loss' in perf]

        if accuracies:
            metrics['final_accuracy'] = accuracies[-1]
            metrics['average_accuracy'] = np.mean(accuracies)
            metrics['min_accuracy'] = np.min(accuracies)
            metrics['max_accuracy'] = np.max(accuracies)

            # è®¡ç®—å‡†ç¡®ç‡ç¨³å®šæ€§
            metrics['accuracy_std'] = np.std(accuracies)
        else:
            metrics.update({
                'final_accuracy': 0,
                'average_accuracy': 0,
                'min_accuracy': 0,
                'max_accuracy': 0,
                'accuracy_std': 0
            })

        # é—å¿˜åº¦é‡
        if len(accuracies) > 1:
            forgetting = 0.0
            for i in range(1, len(accuracies)):
                forgetting += max(0, accuracies[i-1] - accuracies[i])
            metrics['forgetting'] = forgetting / (len(accuracies) - 1)
        else:
            metrics['forgetting'] = 0.0

        # æŸå¤±æŒ‡æ ‡
        if losses:
            metrics['final_loss'] = losses[-1]
            metrics['average_loss'] = np.mean(losses)
            metrics['min_loss'] = np.min(losses)
            metrics['max_loss'] = np.max(losses)
        else:
            metrics.update({
                'final_loss': 0,
                'average_loss': 0,
                'min_loss': 0,
                'max_loss': 0
            })

        # ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡
        if communication_costs:
            metrics['total_communication_cost'] = np.sum(communication_costs)
            metrics['average_communication_cost'] = np.mean(communication_costs)
        else:
            metrics.update({
                'total_communication_cost': 0,
                'average_communication_cost': 0
            })

        if time_history:
            metrics['total_training_time'] = np.sum(time_history)
            metrics['average_time_per_session'] = np.mean(time_history)
            metrics['max_time_per_session'] = np.max(time_history)
        else:
            metrics.update({
                'total_training_time': 0,
                'average_time_per_session': 0,
                'max_time_per_session': 0
            })

        return metrics

    def plot_results(self, performance_history, algorithm_name, save_plot=True):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # å‡†ç¡®ç‡æ›²çº¿
        sessions = range(len(performance_history))
        accuracies = [perf.get('accuracy', 0) for perf in performance_history]
        losses = [perf.get('loss', 0) for perf in performance_history]

        ax1.plot(sessions, accuracies, 'b-', linewidth=2, marker='o')
        ax1.set_xlabel('Training Session')
        ax1.set_ylabel('Accuracy')
        ax1.set_title(f'{algorithm_name} - Model Accuracy Over Time')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)  # å‡†ç¡®ç‡èŒƒå›´0-1

        # æŸå¤±æ›²çº¿
        ax2.plot(sessions, losses, 'r-', linewidth=2, marker='s')
        ax2.set_xlabel('Training Session')
        ax2.set_ylabel('Loss')
        ax2.set_title(f'{algorithm_name} - Model Loss Over Time')
        ax2.grid(True, alpha=0.3)

        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidences = []
        for perf in performance_history:
            if 'confidence' in perf and perf['confidence']:
                confidences.extend(perf['confidence'])

        if confidences:
            ax3.hist(confidences, bins=20, alpha=0.7, edgecolor='black', color='green')
            ax3.set_xlabel('Confidence')
            ax3.set_ylabel('Frequency')
            ax3.set_title(f'{algorithm_name} - Confidence Distribution')
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'No confidence data',
                    horizontalalignment='center', verticalalignment='center',
                    transform=ax3.transAxes, fontsize=12)
            ax3.set_title(f'{algorithm_name} - Confidence Distribution')

        # ç¼“å­˜ä½¿ç”¨æƒ…å†µ
        cache_sizes = []
        for perf in performance_history:
            if 'cache_stats' in perf and perf['cache_stats']:
                total_size = sum(stats.get('total_size', 0) for stats in perf['cache_stats'].values())
                cache_sizes.append(total_size)

        if cache_sizes:
            ax4.plot(range(len(cache_sizes)), cache_sizes, 'g-', linewidth=2, marker='^')
            ax4.set_xlabel('Training Session')
            ax4.set_ylabel('Total Cache Size')
            ax4.set_title(f'{algorithm_name} - Cache Usage Over Time')
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'No cache data',
                    horizontalalignment='center', verticalalignment='center',
                    transform=ax4.transAxes, fontsize=12)
            ax4.set_title(f'{algorithm_name} - Cache Usage Over Time')

        plt.tight_layout()

        if save_plot:
            plot_path = os.path.join(self.save_dir, f"{algorithm_name}_results.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ç»“æœå›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")

        plt.show()
        return fig

    def plot_comparison(self, algorithms_results, save_plot=True):
        """æ¯”è¾ƒä¸åŒç®—æ³•çš„æ€§èƒ½"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()

        # å‡†ç¡®ç‡æ¯”è¾ƒ
        for algo_name, results in algorithms_results.items():
            accuracies = [perf.get('accuracy', 0) for perf in results['performance_history']]
            sessions = range(len(accuracies))
            axes[0].plot(sessions, accuracies, label=algo_name, linewidth=2)

        axes[0].set_xlabel('Training Session')
        axes[0].set_ylabel('Accuracy')
        axes[0].set_title('Accuracy Comparison')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # æŸå¤±æ¯”è¾ƒ
        for algo_name, results in algorithms_results.items():
            losses = [perf.get('loss', 0) for perf in results['performance_history']]
            sessions = range(len(losses))
            axes[1].plot(sessions, losses, label=algo_name, linewidth=2)

        axes[1].set_xlabel('Training Session')
        axes[1].set_ylabel('Loss')
        axes[1].set_title('Loss Comparison')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        # é€šä¿¡æˆæœ¬æ¯”è¾ƒ
        algo_names = []
        comm_costs = []
        for algo_name, results in algorithms_results.items():
            if 'communication_costs' in results:
                algo_names.append(algo_name)
                comm_costs.append(np.sum(results['communication_costs']))

        if comm_costs:
            bars = axes[2].bar(algo_names, comm_costs, alpha=0.7)
            axes[2].set_xlabel('Algorithm')
            axes[2].set_ylabel('Total Communication Cost')
            axes[2].set_title('Communication Cost Comparison')
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, cost in zip(bars, comm_costs):
                axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                           f'{cost:.0f}', ha='center', va='bottom')

        # è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
        algo_names = []
        train_times = []
        for algo_name, results in algorithms_results.items():
            if 'time_history' in results and results['time_history']:
                algo_names.append(algo_name)
                train_times.append(np.sum(results['time_history']))

        if train_times:
            bars = axes[3].bar(algo_names, train_times, alpha=0.7, color='orange')
            axes[3].set_xlabel('Algorithm')
            axes[3].set_ylabel('Total Training Time (s)')
            axes[3].set_title('Training Time Comparison')
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, time_val in zip(bars, train_times):
                axes[3].text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                           f'{time_val:.1f}s', ha='center', va='bottom')

        plt.tight_layout()

        if save_plot:
            plot_path = os.path.join(self.save_dir, "algorithm_comparison.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ç®—æ³•æ¯”è¾ƒå›¾å·²ä¿å­˜è‡³: {plot_path}")

        plt.show()
        return fig

    def print_detailed_metrics(self, metrics, algorithm_name):
        """æ‰“å°è¯¦ç»†çš„æŒ‡æ ‡æŠ¥å‘Š"""
        print(f"\n{'='*50}")
        print(f"{algorithm_name} è¯¦ç»†æŒ‡æ ‡æŠ¥å‘Š")
        print(f"{'='*50}")

        print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {metrics.get('final_accuracy', 0):.4f}")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {metrics.get('average_accuracy', 0):.4f}")
        print(f"  å‡†ç¡®ç‡èŒƒå›´: {metrics.get('min_accuracy', 0):.4f} - {metrics.get('max_accuracy', 0):.4f}")
        print(f"  å‡†ç¡®ç‡æ ‡å‡†å·®: {metrics.get('accuracy_std', 0):.4f}")
        print(f"  é—å¿˜åº¦é‡: {metrics.get('forgetting', 0):.4f}")

        print(f"\nâš¡ ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡:")
        print(f"  æ€»é€šä¿¡æˆæœ¬: {metrics.get('total_communication_cost', 0):.0f}")
        print(f"  å¹³å‡é€šä¿¡æˆæœ¬: {metrics.get('average_communication_cost', 0):.2f}")
        print(f"  æ€»è®­ç»ƒæ—¶é—´: {metrics.get('total_training_time', 0):.2f}s")
        print(f"  å¹³å‡æ¯è½®æ—¶é—´: {metrics.get('average_time_per_session', 0):.2f}s")

        print(f"\nğŸ“ˆ æŸå¤±æŒ‡æ ‡:")
        print(f"  æœ€ç»ˆæŸå¤±: {metrics.get('final_loss', 0):.4f}")
        print(f"  å¹³å‡æŸå¤±: {metrics.get('average_loss', 0):.4f}")
        print(f"  æŸå¤±èŒƒå›´: {metrics.get('min_loss', 0):.4f} - {metrics.get('max_loss', 0):.4f}")