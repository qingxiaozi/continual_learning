import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import os


class ResultVisualizer:
    """ç»“æœå¯è§†åŒ–ç±»"""

    def __init__(self, save_dir="./results"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use("seaborn-v0_8")
        sns.set_palette("husl")

    def plot_training_loss(
        self, epoch_losses, save_plot=True, plot_name="training_loss.png"
    ):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
        plt.figure(figsize=(10, 6))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        epochs = range(1, len(epoch_losses) + 1)
        plt.plot(epochs, epoch_losses, "b-", linewidth=2, label="Training Loss")
        plt.scatter(epochs, epoch_losses, color="red", s=30, zorder=5)

        # è®¾ç½®å›¾è¡¨å±æ€§
        plt.title("Training Loss vs Epochs", fontsize=14, fontweight="bold")
        plt.xlabel("Epoch", fontsize=12)
        plt.ylabel("Loss", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=10)

        # è®¾ç½®xè½´ä¸ºæ•´æ•°
        plt.xticks(epochs)

        # è‡ªåŠ¨è°ƒæ•´yè½´èŒƒå›´ï¼Œç¡®ä¿èƒ½çœ‹æ¸…ä¸‹é™è¶‹åŠ¿
        if len(epoch_losses) > 1:
            loss_range = max(epoch_losses) - min(epoch_losses)
            plt.ylim(
                min(epoch_losses) - 0.1 * loss_range,
                max(epoch_losses) + 0.1 * loss_range,
            )

        # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
        if save_plot:
            plot_path = os.path.join(self.save_dir, plot_name)
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            print(f"è®­ç»ƒæŸå¤±å›¾å·²ä¿å­˜è‡³: {plot_path}")

        plt.close()

        # æ‰“å°è®­ç»ƒæ€»ç»“
        if len(epoch_losses) > 1:
            self._print_training_summary(epoch_losses)

    def _print_training_summary(self, epoch_losses):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        print(f"\nè®­ç»ƒæ€»ç»“:")
        print(f"åˆå§‹æŸå¤±: {epoch_losses[0]:.4f}")
        print(f"æœ€ç»ˆæŸå¤±: {epoch_losses[-1]:.4f}")
        print(f"æŸå¤±ä¸‹é™: {epoch_losses[0] - epoch_losses[-1]:.4f}")
        print(
            f"ä¸‹é™ç™¾åˆ†æ¯”: {(epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100:.2f}%"
        )

    def calculate_metrics(
        self, performance_history, time_history=None, communication_costs=None
    ):
        """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}

        if time_history is None:
            time_history = []
        if communication_costs is None:
            communication_costs = []

        # æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        accuracies = [
            perf["accuracy"] for perf in performance_history if "accuracy" in perf
        ]
        losses = [perf["loss"] for perf in performance_history if "loss" in perf]

        if accuracies:
            metrics["final_accuracy"] = accuracies[-1]
            metrics["average_accuracy"] = np.mean(accuracies)
            metrics["min_accuracy"] = np.min(accuracies)
            metrics["max_accuracy"] = np.max(accuracies)

            # è®¡ç®—å‡†ç¡®ç‡ç¨³å®šæ€§
            metrics["accuracy_std"] = np.std(accuracies)
        else:
            metrics.update(
                {
                    "final_accuracy": 0,
                    "average_accuracy": 0,
                    "min_accuracy": 0,
                    "max_accuracy": 0,
                    "accuracy_std": 0,
                }
            )

        # é—å¿˜åº¦é‡
        if len(accuracies) > 1:
            forgetting = 0.0
            for i in range(1, len(accuracies)):
                forgetting += max(0, accuracies[i - 1] - accuracies[i])
            metrics["forgetting"] = forgetting / (len(accuracies) - 1)
        else:
            metrics["forgetting"] = 0.0

        # æŸå¤±æŒ‡æ ‡
        if losses:
            metrics["final_loss"] = losses[-1]
            metrics["average_loss"] = np.mean(losses)
            metrics["min_loss"] = np.min(losses)
            metrics["max_loss"] = np.max(losses)
        else:
            metrics.update(
                {"final_loss": 0, "average_loss": 0, "min_loss": 0, "max_loss": 0}
            )

        # ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡
        if communication_costs:
            metrics["total_communication_cost"] = np.sum(communication_costs)
            metrics["average_communication_cost"] = np.mean(communication_costs)
        else:
            metrics.update(
                {"total_communication_cost": 0, "average_communication_cost": 0}
            )

        if time_history:
            metrics["total_training_time"] = np.sum(time_history)
            metrics["average_time_per_session"] = np.mean(time_history)
            metrics["max_time_per_session"] = np.max(time_history)
        else:
            metrics.update(
                {
                    "total_training_time": 0,
                    "average_time_per_session": 0,
                    "max_time_per_session": 0,
                }
            )

        return metrics

    def plot_results(self, performance_history, algorithm_name, save_plot=True):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # å‡†ç¡®ç‡æ›²çº¿
        sessions = range(len(performance_history))
        accuracies = [perf.get("accuracy", 0) for perf in performance_history]
        losses = [perf.get("loss", 0) for perf in performance_history]

        ax1.plot(sessions, accuracies, "b-", linewidth=2, marker="o")
        ax1.set_xlabel("Training Session")
        ax1.set_ylabel("Accuracy")
        ax1.set_title(f"{algorithm_name} - Model Accuracy Over Time")
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)  # å‡†ç¡®ç‡èŒƒå›´0-1

        # æŸå¤±æ›²çº¿
        ax2.plot(sessions, losses, "r-", linewidth=2, marker="s")
        ax2.set_xlabel("Training Session")
        ax2.set_ylabel("Loss")
        ax2.set_title(f"{algorithm_name} - Model Loss Over Time")
        ax2.grid(True, alpha=0.3)

        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidences = []
        for perf in performance_history:
            if "confidence" in perf and perf["confidence"]:
                confidences.extend(perf["confidence"])

        if confidences:
            ax3.hist(confidences, bins=20, alpha=0.7, edgecolor="black", color="green")
            ax3.set_xlabel("Confidence")
            ax3.set_ylabel("Frequency")
            ax3.set_title(f"{algorithm_name} - Confidence Distribution")
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(
                0.5,
                0.5,
                "No confidence data",
                horizontalalignment="center",
                verticalalignment="center",
                transform=ax3.transAxes,
                fontsize=12,
            )
            ax3.set_title(f"{algorithm_name} - Confidence Distribution")

        # ç¼“å­˜ä½¿ç”¨æƒ…å†µ
        cache_sizes = []
        for perf in performance_history:
            if "cache_stats" in perf and perf["cache_stats"]:
                total_size = sum(
                    stats.get("total_size", 0) for stats in perf["cache_stats"].values()
                )
                cache_sizes.append(total_size)

        if cache_sizes:
            ax4.plot(
                range(len(cache_sizes)), cache_sizes, "g-", linewidth=2, marker="^"
            )
            ax4.set_xlabel("Training Session")
            ax4.set_ylabel("Total Cache Size")
            ax4.set_title(f"{algorithm_name} - Cache Usage Over Time")
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(
                0.5,
                0.5,
                "No cache data",
                horizontalalignment="center",
                verticalalignment="center",
                transform=ax4.transAxes,
                fontsize=12,
            )
            ax4.set_title(f"{algorithm_name} - Cache Usage Over Time")

        plt.tight_layout()

        if save_plot:
            plot_path = os.path.join(self.save_dir, f"{algorithm_name}_results.png")
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            print(f"ç»“æœå›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")

        plt.show()
        return fig

    def plot_comparison(self, algorithms_results, save_plot=True):
        """æ¯”è¾ƒä¸åŒç®—æ³•çš„æ€§èƒ½"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()

        # å‡†ç¡®ç‡æ¯”è¾ƒ
        for algo_name, results in algorithms_results.items():
            accuracies = [
                perf.get("accuracy", 0) for perf in results["performance_history"]
            ]
            sessions = range(len(accuracies))
            axes[0].plot(sessions, accuracies, label=algo_name, linewidth=2)

        axes[0].set_xlabel("Training Session")
        axes[0].set_ylabel("Accuracy")
        axes[0].set_title("Accuracy Comparison")
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # æŸå¤±æ¯”è¾ƒ
        for algo_name, results in algorithms_results.items():
            losses = [perf.get("loss", 0) for perf in results["performance_history"]]
            sessions = range(len(losses))
            axes[1].plot(sessions, losses, label=algo_name, linewidth=2)

        axes[1].set_xlabel("Training Session")
        axes[1].set_ylabel("Loss")
        axes[1].set_title("Loss Comparison")
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        # é€šä¿¡æˆæœ¬æ¯”è¾ƒ
        algo_names = []
        comm_costs = []
        for algo_name, results in algorithms_results.items():
            if "communication_costs" in results:
                algo_names.append(algo_name)
                comm_costs.append(np.sum(results["communication_costs"]))

        if comm_costs:
            bars = axes[2].bar(algo_names, comm_costs, alpha=0.7)
            axes[2].set_xlabel("Algorithm")
            axes[2].set_ylabel("Total Communication Cost")
            axes[2].set_title("Communication Cost Comparison")
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, cost in zip(bars, comm_costs):
                axes[2].text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height(),
                    f"{cost:.0f}",
                    ha="center",
                    va="bottom",
                )

        # è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
        algo_names = []
        train_times = []
        for algo_name, results in algorithms_results.items():
            if "time_history" in results and results["time_history"]:
                algo_names.append(algo_name)
                train_times.append(np.sum(results["time_history"]))

        if train_times:
            bars = axes[3].bar(algo_names, train_times, alpha=0.7, color="orange")
            axes[3].set_xlabel("Algorithm")
            axes[3].set_ylabel("Total Training Time (s)")
            axes[3].set_title("Training Time Comparison")
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, time_val in zip(bars, train_times):
                axes[3].text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height(),
                    f"{time_val:.1f}s",
                    ha="center",
                    va="bottom",
                )

        plt.tight_layout()

        if save_plot:
            plot_path = os.path.join(self.save_dir, "algorithm_comparison.png")
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            print(f"ç®—æ³•æ¯”è¾ƒå›¾å·²ä¿å­˜è‡³: {plot_path}")

        plt.show()
        return fig

    def print_detailed_metrics(self, metrics, algorithm_name):
        """æ‰“å°è¯¦ç»†çš„æŒ‡æ ‡æŠ¥å‘Š"""
        print(f"\n{'='*50}")
        print(f"{algorithm_name} è¯¦ç»†æŒ‡æ ‡æŠ¥å‘Š")
        print(f"{'='*50}")

        print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {metrics.get('final_accuracy', 0):.4f}")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {metrics.get('average_accuracy', 0):.4f}")
        print(
            f"  å‡†ç¡®ç‡èŒƒå›´: {metrics.get('min_accuracy', 0):.4f} - {metrics.get('max_accuracy', 0):.4f}"
        )
        print(f"  å‡†ç¡®ç‡æ ‡å‡†å·®: {metrics.get('accuracy_std', 0):.4f}")
        print(f"  é—å¿˜åº¦é‡: {metrics.get('forgetting', 0):.4f}")

        print(f"\nâš¡ ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡:")
        print(f"  æ€»é€šä¿¡æˆæœ¬: {metrics.get('total_communication_cost', 0):.0f}")
        print(f"  å¹³å‡é€šä¿¡æˆæœ¬: {metrics.get('average_communication_cost', 0):.2f}")
        print(f"  æ€»è®­ç»ƒæ—¶é—´: {metrics.get('total_training_time', 0):.2f}s")
        print(f"  å¹³å‡æ¯è½®æ—¶é—´: {metrics.get('average_time_per_session', 0):.2f}s")

        print(f"\nğŸ“ˆ æŸå¤±æŒ‡æ ‡:")
        print(f"  æœ€ç»ˆæŸå¤±: {metrics.get('final_loss', 0):.4f}")
        print(f"  å¹³å‡æŸå¤±: {metrics.get('average_loss', 0):.4f}")
        print(
            f"  æŸå¤±èŒƒå›´: {metrics.get('min_loss', 0):.4f} - {metrics.get('max_loss', 0):.4f}"
        )

    def plot_data_heterogeneity(
        self, data_simulator, session, save_plot=True, plot_name=None
    ):
        """
        ç»˜åˆ¶æ•°æ®å¼‚è´¨æ€§ç¤ºæ„å›¾

        å‚æ•°:
            data_simulator: DomainIncrementalDataSimulatorå®ä¾‹
            session: å½“å‰ä¼šè¯ID
            save_plot: æ˜¯å¦ä¿å­˜å›¾ç‰‡
            plot_name: å›¾ç‰‡åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if plot_name is None:
            plot_name = f"data_heterogeneity_session_{session}.png"

        # è·å–å½“å‰åŸŸçš„ä¿¡æ¯
        current_domain = data_simulator.get_current_domain()
        domain_key = f"{data_simulator.current_dataset}_{current_domain}"

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åˆ†é…
        if domain_key not in data_simulator.vehicle_data_assignments:
            print(f"è­¦å‘Š: åŸŸ {domain_key} æ²¡æœ‰æ•°æ®åˆ†é…ä¿¡æ¯")
            return

        # è·å–ç±»åˆ«ä¿¡æ¯
        num_classes = data_simulator.dataset_info[data_simulator.current_dataset][
            "num_classes"
        ]
        class_labels = [f"Class {i}" for i in range(num_classes)]

        # è·å–è½¦è¾†åˆ†é…æ•°æ®
        vehicle_assignments = data_simulator.vehicle_data_assignments[domain_key]
        train_dataset = data_simulator.train_data_cache[domain_key]

        # ç»Ÿè®¡æ¯ä¸ªè½¦è¾†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        vehicle_class_counts = {}

        for vehicle_id, indices in vehicle_assignments.items():
            class_counts = {i: 0 for i in range(num_classes)}

            for idx in indices:
                # è·å–æ ·æœ¬çš„æ ‡ç­¾
                _, label = train_dataset[idx]
                class_counts[label] += 1

            vehicle_class_counts[vehicle_id] = class_counts

        # å‡†å¤‡ç»˜å›¾æ•°æ®
        vehicle_ids = []
        class_ids = []
        sample_counts = []

        for vehicle_id in range(data_simulator.num_vehicles):
            if vehicle_id in vehicle_class_counts:
                for class_id in range(num_classes):
                    count = vehicle_class_counts[vehicle_id][class_id]
                    if count > 0:  # åªç»˜åˆ¶æœ‰æ ·æœ¬çš„ç±»åˆ«
                        vehicle_ids.append(vehicle_id)
                        class_ids.append(class_id)
                        sample_counts.append(count)

        if not sample_counts:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¯ç»˜åˆ¶çš„æ•°æ®")
            return

        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(12, 8))

        # åˆ›å»ºæ•£ç‚¹å›¾ï¼Œç‚¹çš„å¤§å°è¡¨ç¤ºæ ·æœ¬æ•°é‡
        scatter = plt.scatter(
            vehicle_ids,
            class_ids,
            s=[
                min(100 + count * 2, 500) for count in sample_counts
            ],  # åŠ¨æ€è°ƒæ•´ç‚¹çš„å¤§å°
            c=sample_counts,
            cmap="viridis",
            alpha=0.7,
            edgecolors="black",
            linewidth=0.5,
        )

        # è®¾ç½®å›¾è¡¨å±æ€§
        plt.title(
            f"Data Heterogeneity - Session {session}\n(Domain: {current_domain}, Dataset: {data_simulator.current_dataset})",
            fontsize=14,
            fontweight="bold",
            pad=20,
        )
        plt.xlabel("Vehicle ID", fontsize=12)
        plt.ylabel("Class Label", fontsize=12)

        # è®¾ç½®åæ ‡è½´
        plt.xticks(range(data_simulator.num_vehicles))
        plt.yticks(range(num_classes), class_labels)
        plt.grid(True, alpha=0.3, linestyle="--")

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, shrink=0.8)
        cbar.set_label("Number of Samples", fontsize=10)

        # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡æ³¨ï¼ˆåªæ ‡æ³¨è¾ƒå¤§çš„ç‚¹ï¼‰
        for i, (vehicle_id, class_id, count) in enumerate(
            zip(vehicle_ids, class_ids, sample_counts)
        ):
            if count > max(sample_counts) * 0.3:  # åªæ ‡æ³¨è¾ƒå¤§çš„æ ·æœ¬ç‚¹
                plt.annotate(
                    str(count),
                    (vehicle_id, class_id),
                    xytext=(5, 5),
                    textcoords="offset points",
                    fontsize=8,
                    ha="left",
                    va="bottom",
                )

        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()

        # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
        if save_plot:
            plot_path = os.path.join(self.save_dir, plot_name)
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            print(f"æ•°æ®å¼‚è´¨æ€§å›¾å·²ä¿å­˜è‡³: {plot_path}")

        plt.close()

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        # self._print_heterogeneity_statistics(vehicle_class_counts, current_domain, session)

    def _print_heterogeneity_statistics(self, vehicle_class_counts, domain, session):
        """æ‰“å°æ•°æ®å¼‚è´¨æ€§ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n=== Session {session} - {domain} æ•°æ®å¼‚è´¨æ€§ç»Ÿè®¡ ===")

        total_samples = 0
        class_coverage = {}  # æ¯ä¸ªç±»åˆ«è¢«å¤šå°‘è½¦è¾†è¦†ç›–

        for vehicle_id, class_counts in vehicle_class_counts.items():
            vehicle_total = sum(class_counts.values())
            total_samples += vehicle_total

            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„è¦†ç›–æƒ…å†µ
            for class_id, count in class_counts.items():
                if count > 0:
                    class_coverage[class_id] = class_coverage.get(class_id, 0) + 1

            print(
                f"è½¦è¾† {vehicle_id}: {vehicle_total} ä¸ªæ ·æœ¬, è¦†ç›– {sum(1 for c in class_counts.values() if c > 0)} ä¸ªç±»åˆ«"
            )

        # è®¡ç®—å¼‚è´¨æ€§æŒ‡æ ‡
        vehicle_totals = [
            sum(counts.values()) for counts in vehicle_class_counts.values()
        ]
        heterogeneity_std = np.std(vehicle_totals) if vehicle_totals else 0

        print(f"\næ€»ä½“ç»Ÿè®¡:")
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"å¹³å‡æ¯è½¦æ ·æœ¬æ•°: {np.mean(vehicle_totals):.1f}")
        print(f"æ ·æœ¬æ•°æ ‡å‡†å·®: {heterogeneity_std:.1f} (å¼‚è´¨æ€§æŒ‡æ ‡)")
        print(
            f"ç±»åˆ«è¦†ç›–æƒ…å†µ: å¹³å‡æ¯ä¸ªç±»åˆ«è¢« {np.mean(list(class_coverage.values())):.1f} è¾†è½¦è¦†ç›–"
        )
        print("====================================\n")
